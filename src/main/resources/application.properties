# ==================== 服务器配置 ====================
server.port=8080

# ==================== 数据库配置 ====================
# MySQL 数据源配置
spring.datasource.url=jdbc:mysql://localhost:3306/12306_ticket
spring.datasource.username=root
spring.datasource.password=root
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver

# R2DBC 配置
spring.r2dbc.url=r2dbc:mysql://localhost:3306/12306_ticket
spring.r2dbc.username=root
spring.r2dbc.password=root
spring.r2dbc.pool.initial-size=5
spring.r2dbc.pool.max-size=20

# MongoDB 配置
spring.data.mongodb.uri=mongodb://localhost:27017/chat_memory_db

# MyBatis Plus 配置
mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl

# ==================== AI模型配置 ====================
# Ollama 配置
langchain4j.ollama.chat-model.base-url=http://localhost:11434
langchain4j.ollama.chat-model.model-name=deepseek-r1:8b
langchain4j.ollama.chat-model.log-requests=true
langchain4j.ollama.chat-model.log-responses=true

# 阿里云百炼平台配置
langchain4j.community.dashscope.chat-model.api-key=${DASH_SCOPE_API_KEY}
langchain4j.community.dashscope.chat-model.model-name=qwen-max

# 百炼-DeepSeek 配置
langchain4j.open-ai.chat-model.base-url=https://dashscope.aliyuncs.com/compatible-mode/v1
langchain4j.open-ai.chat-model.api-key=${DASH_SCOPE_API_KEY}
langchain4j.open-ai.chat-model.model-name=deepseek-v3
# 温度系数：取值范围通常在 0 到 1 之间。值越高，模型的输出越随机、富有创造性；
# 值越低，输出越确定、保守。这里设置为 0.9，意味着模型会有一定的随机性，生成的回复可能会比较多样化。
langchain4j.open-ai.chat-model.temperature=0.9

# 阿里通义千问-通用文本向量-v3
langchain4j.community.dashscope.embedding-model.api-key=${DASH_SCOPE_API_KEY}
langchain4j.community.dashscope.embedding-model.model-name=text-embedding-v3

# 阿里通义千问-流式输出
langchain4j.community.dashscope.streaming-chat-model.api-key=${DASH_SCOPE_API_KEY}
langchain4j.community.dashscope.streaming-chat-model.model-name=qwen-plus

# ==================== MCP配置 ====================
mcp.server.base-url=http://127.0.0.1:${server.port}/sse
mcp.client.log-requests=true
mcp.client.log-responses=true